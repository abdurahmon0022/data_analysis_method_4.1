{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04gradient_descen.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8xLIBsuaom4"
      },
      "source": [
        "# **Задание 1**\n",
        "\n",
        "Реализуйте класс LinearRegressionSGD c обучением и и применением линейной регрессии, построенной с помощью стохастического градиентного спуска, с заданным интерфейсом.\n",
        "\n",
        "Обратите внимание на следуюшие моменты:\n",
        "\n",
        "Схожий класс использовался в лекции\n",
        "\n",
        "Выбирайте 10 случайных сэмплов (равномерно) каждый раз.\n",
        "\n",
        "Используйте параметры по умолчанию (epsilon=1e-6, max_steps=10000, w0=None, alpha=1e-8)\n",
        "\n",
        "Выход из цикла осуществуется по сравнению 2-нормы разницы весов с epsilon, а функция потерь - MSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJvw1rMpVCrA"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "np.random.seed(0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKVvUH_zV5nA"
      },
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class LinearRegressionSGD(BaseEstimator):\n",
        "    def __init__(self, epsilon=1e-6, max_steps=10000, w0=None, alpha=1e-8):\n",
        "        self.epsilon = epsilon\n",
        "        self.max_steps = max_steps\n",
        "        self.w0 = w0\n",
        "        self.alpha = alpha\n",
        "        self.w = None\n",
        "        self.w_history = []\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUp401sTXbee"
      },
      "source": [
        "def fit(self, X, y):\n",
        "        l,d = X.shape \n",
        "        if self.w0 is None:\n",
        "            self.w0 = np.zeros(d)\n",
        "        self.w = self.w0\n",
        "        for step in range(self.max_steps):\n",
        "            self.w_history.append(self.w)\n",
        "            w_new = self.w - self.alpha * self.calc_gradient(X, y)\n",
        "            if (np.linalg.norm(w_new - self.w) < self.epsilon):\n",
        "                break\n",
        "            self.w = w_new\n",
        "        return self\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLJeUudJXZZS"
      },
      "source": [
        "def predict(self, X):\n",
        "        if self.w is None:\n",
        "            raise Exception(\"Пока не обучена\")  \n",
        "        y_pred = np.dot(X, self.w)\n",
        "        return y_pred\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMk9herBXWbK"
      },
      "source": [
        "def calc_gradient(self, X, y):\n",
        "        l, d = X.shape\n",
        "        gradient = np.zeros((d, ))\n",
        "        indeces = np.random.randint(0, d, (10, ))\n",
        "        return (2/l) * np.dot(X.T,(np.dot(X, self.w) - y))"
      ],
      "execution_count": 27,
      "outputs": []
    }
  ]
}